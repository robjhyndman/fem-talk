---
title: Hierarchical Time&nbsp;Series Forecasting in Emergency Medical Services
author: Rob J Hyndman\newline Bahman Rostami-Tabar
pdf-engine: pdflatex
fig-width: 9
fig-height: 4.5
format:
  beamer:
    theme: monash
    aspectratio: 169
    fontsize: 14pt
    section-titles: false
    knitr:
      opts_chunk:
        dev: "CairoPDF"
include-in-header: header.tex
keep-tex: true
toc: false
execute:
  echo: false
  message: false
  warning: false
  cache: true
---

```{r}
#| label: setup
#| cache: false
source(here::here("setup.R"))
```


## Wales Health Board Areas

\placefig{3.3}{1.2}{width=7.8cm}{Map-of-Wales-Health-Boards}

## Data

```{r}
#| label: data
incident_gthf <- readr::read_rds(here::here("data/incidents_gt.rds"))
```



* Daily number of attended incidents:\newline 1 October 2015 -- 31 July 2019
* Disaggregated by:
  * control area
  * health board
  * priority
  * nature of incidents
* `r label_comma()(NROW(incident_gthf))` rows observations from `r label_comma()(NROW(attributes(incident_gthf)$key))` time series.

## Data structure

```{r}
#| label: data_structure
#| include: false
data <- data.frame(
  level1 = "Total",
  level2 = c(
    "Central & West", "Central & West", "Central & West",
    "North", "South & East", "South & East", "South & East"
  ),
  level3 = c("HD", "AB", "PO", "BC", "CV", "CT", "AB")
)
# transform it to a edge list!
edges_level1_2 <- data |>
  select(level1, level2) |>
  unique() |>
  rename(from = level1, to = level2)
edges_level2_3 <- data |>
  select(level2, level3) |>
  unique() |>
  rename(from = level2, to = level3)
Cairo::CairoPDF(here::here("figs/group.pdf"), width = 6, height = 6)
rbind(edges_level1_2, edges_level2_3) |>
  igraph::graph_from_data_frame() |>
  ggraph::ggraph(layout = "dendrogram", circular = FALSE) +
  ggraph::geom_edge_diagonal() +
  ggraph::geom_node_point(color = "#dddddd", size = 10) +
  ggraph::geom_node_text(
    aes(label = c(
      "All country",
      "Central & West", "North", "South & East",
      "HD", "AB", "PO", "BC", "CV", "CT", "AB"
    ))
  ) +
  theme_void() +
  theme(
    # panel.background = element_rect(fill = "transparent"), # transparent panel bg
    plot.background = element_rect(fill = "transparent"), # transparent plot bg
  )
crop::dev.off.crop()
```

\placefig{0.3}{1.5}{width=7cm}{group.pdf}
\placefig{7.5}{1.5}{width=7.7cm}{group.png}

## Data structure
\fontsize{10}{11}\sf

```{r}
#| label: data_structure_table
agg_level <- tibble::tribble(
  ~Level, ~`Number of series`,
  "All country", 1,
  "Control", 3,
  "Health board", 7,
  "Priority", 3,
  "Priority * Control", 9,
  "Priority * Health board", 21,
  "Nature of incident", 35,
  "Nature of incident * Control", 105,
  "Nature of incident * Health board", 245,
  "Priority * Nature of incident", 104,
  "Control * Priority * Nature of incident", 306,
  "Control * Health board * Priority * Nature of incident (Bottom level)", 691,
  "Total", 1530
)
knitr::kable(agg_level, booktabs = TRUE, position = "left", linesep = "") |>
  kable_classic(full_width = FALSE)
```

## Data
\fontsize{10}{10}\sf

```{r}
incident_gthf
```

## Data
\fontsize{10}{10}\sf

```{r}
incident_gthf  |>
  arrange(date, region, category, nature, lhb)  |>
  mutate(category = recode(category, RED = "Red", AMB = "Amber", GRE = "Green"))
```

```{r}
#| label: time_plots
gglabs <- ggplot2::labs(x = "Date", y = "Incidents")
p_total <- incident_gthf |>
  filter(is_aggregated(region) & is_aggregated(lhb) & is_aggregated(category) & is_aggregated(nature)) |>
  autoplot(incident) +
  gglabs

p_control <- incident_gthf |>
  filter(!is_aggregated(region) & !is_aggregated(lhb) & is_aggregated(category) & is_aggregated(nature)) |>
  as_tibble() |>
  select(-nature, -category) |>
  group_by(date, region) |>
  summarise(incident = sum(incident), .groups = "drop") |>
  ggplot(aes(x = date, y = incident, color = factor(region))) +
  geom_line() +
  gglabs +
  labs(color = "Control")

p_board <- incident_gthf |>
  filter(!is_aggregated(region) & !is_aggregated(lhb) & is_aggregated(category) & is_aggregated(nature)) |>
  as_tibble() |>
  select(-nature, -category) |>
  ggplot(aes(x = date, y = incident, color = factor(lhb))) +
  geom_line() +
  gglabs +
  labs(color = "Health board")

p_priority <- incident_gthf |>
  filter(is_aggregated(region) & is_aggregated(lhb) & !is_aggregated(category) & is_aggregated(nature)) |>
  mutate(
    category = recode(category, RED = "Red", AMB = "Amber", GRE = "Green"),
    category = factor(category, levels = c("Red", "Amber", "Green"))
  ) |>
  as_tibble() |>
  select(-nature, -region) |>
  ggplot(aes(x = date, y = incident, color = factor(category))) +
  geom_line() +
  scale_color_manual(values = c(Red = "#ff3300", Amber = "#E69f00", Green = "#009e73")) +
  gglabs +
  labs(color = "Priority")

p_nature1 <- incident_gthf |>
  filter(is_aggregated(region) & is_aggregated(lhb) & is_aggregated(category) & !is_aggregated(nature)) |>
  as_tibble() |>
  mutate(nature = stringr::str_pad(as.character(nature), width = 12, side = "right")) |>
  group_by(date, nature, lhb) |>
  summarise(incident = sum(incident), .groups = "drop") |>
  ggplot(aes(x = date, y = incident, color = nature)) +
  geom_line() +
  gglabs +
  labs(color = "Nature of incident") +
  ylim(0,260)

top_cat <- incident_gthf |>
  as_tibble() |>
  filter(!is_aggregated(nature)) |>
  group_by(nature) |>
  summarise(incident = sum(incident), .groups = "drop") |>
  arrange(desc(incident)) |>
  head(3) |>
  pull(nature) |>
  as.character()
selected_nature <- c("CHESTPAIN", "STROKECVA", "BREATHING", "ABDOMINAL")
p_nature2 <- incident_gthf |>
  filter(is_aggregated(region) & is_aggregated(lhb) & is_aggregated(category) & !is_aggregated(nature)) |>
  as_tibble() |>
  mutate(nature = as.character(nature)) |>
  filter(nature %in% selected_nature) |>
  group_by(date, nature, lhb) |>
  summarise(incident = sum(incident), .groups = "drop") |>
  ggplot(aes(x = date, y = incident, color = nature)) +
  geom_line() +
  gglabs +
  labs(color = "Nature of incident") +
  ylim(0,260)

p <- patchwork::align_patches(p_total, p_control, p_board, p_priority, p_nature1, p_nature2)
```

## Aggregated daily incidents

```{r}
#| label: time_plots1
p[[1]]
```

## Daily incidents by control area

```{r}
#| label: time_plots2
p[[2]]
```

## Data incidents by health board

```{r}
#| label: time_plots3
p[[3]]
```

## Data incidents by priority

```{r}
#| label: time_plots4
p[[4]]
```

## Data incidents by nature of incident

```{r}
#| label: time_plots5
p[[5]]
```

## Data incidents by nature of incident

```{r}
#| label: time_plots6
p[[6]]
```

## Data features

```{r}
#| label: data_features
incident_gthf |>
  features(incident, feat_stl) |>
  ggplot(aes(x = trend_strength, y = seasonal_strength_week)) +
  geom_point(alpha = 0.25) +
  labs(x = "Strength of trend", y = "Strength of weekly seasonality")
```

## Forecasting methods

1. **Naïve**: Empirical distribution of past daily attended incidents.
2. **ETS**: Exponential Smoothing State Space models.
3. **GLM**: Poission Regression with spline trend, day of the week, annual Fourier seasonality, public holidays, school holidays, Christmas Day, New Year's Day.
4. **TSGLM**: Poisson Regression with same covariates plus three autoregressive terms.
5. **Ensemble**: Mixture distribution of 1--4.

## Forecasting methods

1. **Naïve**: Empirical distribution of past daily attended incidents.

\begin{block}{}
\centerline{$y_{T+h|T} \sim \text{Empirical}(y_{1},\dots,y_{T})$}
\end{block}\vspace*{1cm}\pause

2. **ETS**: Exponential Smoothing State Space models.

\begin{block}{}
\centerline{$y_{T+h|T} \sim \text{Normal}(\hat{y}_{T+h|T},\hat{\sigma}^2_{T+h|T})$}
\end{block}

## Forecasting methods

3. **GLM**: Poission Regression

\begin{block}{}
\centerline{$y_{T+h|T} \sim \text{Poisson}(\hat{y}_{T+h|T}) \qquad\text{where}\qquad \hat{y}_{T+h|T} = \exp(\bm{x}_{T+h}'\bm{\beta})$}
\end{block}

and $\bm{x}_{T+h}$ is a vector of covariates including

\begin{multicols}{2}
\begin{itemize}\tightlist
\item spline trend
\item day of the week
\item annual Fourier seasonality
\item public holidays
\item school holidays
\item Christmas Day
\item New Year's Day
\end{itemize}
\end{multicols}\vspace*{10cm}

## Forecasting methods
\fontsize{9}{8}\sf\vspace*{-0.15cm}

```{r}
y <- incident_gthf |>
  filter(is_aggregated(region), is_aggregated(category), is_aggregated(nature))  |>
  pull(incident)  |>
  ts(frequency=7, start=c(1,4))
holidays <- readr::read_rds(here::here("data/holidays_ts.rds"))

n <- length(y)
fourier_year <- forecast::fourier(ts(y, frequency = 365.25), K = 3)
season_week <- forecast::seasonaldummy(y)
trend <- splines::ns(seq(n), df = round(n / 300))
X <- cbind(trend, season_week, fourier_year, holidays[seq_along(y), ])
# Remove constant covariates
constant <- apply(X, 2, forecast:::is.constant)
X <- X[, !constant]
fit <- glm(y ~ X, family = poisson())
names(fit$coefficients) <- c(
  "(Intercept)", paste0("Spline_",1:5),
  "Monday","Tuesday", "Wednesday", "Thursday", "Friday", "Saturday",
  "Fourier_S1_365", "Fourier_C1_365", "Fourier_S2_365", "Fourier_C2_365","Fourier_S3_365", "Fourier_C3_365",
  "Public_holiday","School_holiday", "Xmas", "New_years_day")
glm_summary(fit)
```

\begin{textblock}{2.7}(12,3)
\fontsize{9}{9}\sf
\begin{block}{\fontsize{9}{9}\sf\centerline{Significance}}
\begin{tabular}{rl}
\verb|***| &$p < 0.001$\\
\verb|**| &$p < 0.01$\\
\verb|*| &$p < 0.05$\\
\verb|.| &$p < 0.1$
\end{tabular}
\end{block}
\end{textblock}

## Forecasting methods

4. **TSGLM**: Poisson Regression

\begin{block}{}\vspace*{-0.8cm}
\begin{align*}
y_{T+h|T} &\sim \text{Poisson}(\hat{y}_{T+h|T}) \\
\text{where}\qquad
\hat{y}_{T+h|T} &= \exp\Big(\bm{x}_{T+h}'\bm{\beta} + \sum_{k=1}^3 \alpha_k \log(y_{T+h-k}+1)\Big)
\end{align*}
\end{block}\vspace*{-0.3cm}

and $\bm{x}_{T+h}$ is a vector of covariates including

\begin{multicols}{2}
\begin{itemize}\tightlist
\item spline trend
\item day of the week
\item annual Fourier seasonality
\item public holidays
\item school holidays
\item Christmas Day
\item New Year's Day
\end{itemize}
\end{multicols}\vspace*{10cm}

## Notation

\begin{textblock}{8.5}(0.2,1.5)
Every collection of time series with linear constraints can be written as
\centerline{\colorbox[RGB]{210,210,210}{$\bY_{t}=\color{blue}\bS\color{red}\bm{b}_{t}$}}
\vspace*{-0.9cm}\begin{itemize}\parskip=0cm\itemsep=0cm
\item $\by_t=$ vector of all series at time $t$
\item $ y_{\text{Total},t}= $ aggregate of all series at time
$t$.
\item $ y_{X,t}= $ value of series $X$ at time $t$.
\item $\color{red}{\bm{b}_t}=$ vector of most disaggregated series at time $t$
\item $\color{blue}{\bS}=$ ``summing matrix'' containing the linear constraints.
\end{itemize}
\end{textblock}

\begin{textblock}{5.7}(11.4,0.1)
\begin{minipage}{4cm}
\begin{block}{}\centering
\begin{tikzpicture}
\tikzstyle{every node}=[ellipse,draw,fill=red!15,inner sep=2pt]
\tikzstyle[level distance=.3cm]
\tikzstyle[sibling distance=12cm]
\tikzstyle{level 1}=[sibling distance=10mm,font=\small,set style={{every node}+=[fill=blue!15]}]
\node{Total}[edge from parent fork down]
 child {node {A}
 }
 child {node {B}
 }
 child {node {C}
 };
\end{tikzpicture}
\end{block}
\end{minipage}
\end{textblock}

\only<1>{\begin{textblock}{5.7}(9.4,2.8)\fontsize{14}{15}\sf
\begin{align*}
\bY_{t}&= \begin{pmatrix}
  y_{\text{Total},t}\\
  y_{A,t}\\
  y_{B,t}\\
  y_{C,t}
  \end{pmatrix}  \\
  &= {\color{blue}\underbrace{\begin{pmatrix}
                1 & 1 & 1 \\
                1 & 0 & 0 \\
                0 & 1 & 0\\
                0 & 0 & 1
                \end{pmatrix}}_{\bS}}
     {\color{red}\underbrace{\begin{pmatrix}
       y_{A,t}\\y_{B,t}\\y_{C,t}
       \end{pmatrix}}_{\bm{b}_{t}}}
\end{align*}
\end{textblock}}
\only<2>{\begin{textblock}{5.7}(9.4,3.2)\fontsize{14}{15}\sf
\begin{alertblock}{}
\begin{itemize}\itemsep=0.1cm
\item Base forecasts: $\hat{\bm{y}}_{T+h|T}$
\item Reconciled forecasts: $\tilde{\bm{y}}_{T+h|T}=\bS\bm{G}\hat{\bm{y}}_{T+h|T}$
\item MinT: $\bG = (\bS'\bm{W}_h^{-1}\bS)^{-1}\bS'\bm{W}_h^{-1}$ where $\bm{W}_h$ is covariance matrix of base forecast errors.
\end{itemize}
\end{alertblock}
\end{textblock}}

## Nonparametric bootstrap reconciliation

* Fit model to all series and store the residuals as $\underaccent{\tilde}{\bm{\varepsilon}}_t$.
* These should be serially uncorrelated but cross-sectionally correlated.
* Draw iid samples from $\underaccent{\tilde}{\bm{\varepsilon}}_1,\dots,\underaccent{\tilde}{\bm{\varepsilon}}_T$ with replacement.
* Simulate future sample paths for model using the bootstrapped residuals.
* Reconcile each sample path using MinT.
* Combine the reconciled sample paths to form a mixture distribution at each forecast horizon.

## Performance evaluation

* Ten-fold time series cross-validation
* Forecast horizon of 1--84 days
* Each training set contains an additional 42 days.
* Forecasts at 43--84 days correspond to planning horizon.

```{r}
#| label: cv1
#| echo: false
#| fig-height: 2.7
.lines <- 10
.init <- 30
.step <- 6
h <- 7:12
max_t <- .init + .step*(.lines - 1) + max(h)
expand.grid(
    time = seq(max_t),
    .id = seq(.lines)
  ) |>
  mutate(
    min_h = (.id-1) * .step + .init + min(h),
    max_h =  (.id-1) * .step + .init + max(h),
    observation = case_when(
      time <= ((.id - 1) * .step + .init) ~ "train",
      time < min_h ~ "test1",
      time <= max_h ~ "test2",
      TRUE ~ "unused"
    )
  ) |>
  ggplot(aes(x = time, y = .id)) +
  geom_segment(
    aes(x = 0, xend = max_t+1, y = .id, yend = .id),
    arrow = arrow(length = unit(0.015, "npc")),
    col = "black", linewidth = .25
  ) +
  geom_point(aes(col = observation), size = 2) +
  scale_y_reverse() +
  scale_colour_manual(values = c(train = "#0072B2", test1 = "#d59771", test2 = "#D55E00", unused = "gray")) +
  theme_void() +
  guides(colour = "none") +
  labs(x = "weeks", y = "") +
  theme_void() +
  theme(
    axis.title = element_text(),
    plot.background = element_rect(fill = "#fafafa", color = "#fafafa")
  )
```

## Performance evaluation

\vspace*{0.2cm}\begin{block}{}
\centerline{$\text{MASE} = \text{mean}(|q_{j}|)$}
$$
  q_{j} = \frac{e_{j}\phantom{^2}}
 {\displaystyle\frac{1}{T-m}\sum_{t=m+1}^T |y_{t}-y_{t-m}|}
$$
\end{block}

* $y_t=$ observation for period $t$
* $e_{j}=$ forecast error for forecast horizon $j$
* $T=$ size of training set
* $m = 7$

\vspace*{10cm}

## Performance evaluation

\vspace*{0.2cm}\begin{block}{}
\centerline{$\text{MSSE} = \text{mean}(q_{j}^2)$}
$$
  q^2_{j} = \frac{ e^2_{j}}
 {\displaystyle\frac{1}{T-m}\sum_{t=m+1}^T (y_{t}-y_{t-m})^2}
$$
\end{block}

* $y_t=$ observation for period $t$
* $e_{j}=$ forecast error for forecast horizon $j$
* $T=$ size of training set
* $m = 7$

\vspace*{10cm}

## Performance evaluation

\vspace*{0.2cm}\begin{block}{}
\centerline{$\text{CRPS} = \text{mean}(p_j)$}
$$
  p_j = \int_{-\infty}^{\infty} \left(G_j(x) - F_j(x)\right)^2dx,
$$
\end{block}

* $G_j(x)=$ forecast distribution for forecast horizon $j$
* $F_j(x)=$ true distribution for same period

\vspace*{10cm}

```{r}
#| label: results
#| include: false
rmsse <- readr::read_rds(here::here("results/rmsse.rds")) |>
  mutate(msse = rmsse^2)
mase <- readr::read_rds(here::here("results/mase.rds"))
crps <- readr::read_rds(here::here("results/crps.rds")) |>
  pivot_wider(names_from = model, values_from = crps) |>
  group_by(method, h, series) |>
  mutate(across(where(is.numeric), ~ .x / naiveecdf)) |>
  ungroup()
```


## Forecast accuracy

```{r}
#| label: results_graph
crps <- crps |>
  pivot_longer(ensemble:tscount, names_to = "model", values_to = "crps")

accuracy <- bind_rows(
  rmsse |> mutate(measure = "msse", accuracy = rmsse^2),
  mase |> mutate(measure = "mase", accuracy = mase),
  crps |> mutate(measure = "crps", accuracy = crps),
) |>
  select(-rmsse, -mase, -crps)

# Plot of average accuracy vs week for each method for Total
acc_summary <- accuracy |>
  filter(
    series %in% c("Total", "Control areas", "Health boards"),
    method == "mint", model != "qcomb"
  ) |>
  mutate(model = case_when(
    model == "naiveecdf" ~ "Naïve",
    model == "ets" ~ "ETS",
    model == "tscount" ~ "TSGLM",
    model == "iglm" ~ "GLM",
    model == "ensemble" ~ "Ensemble"
  )) |>
  mutate(
    measure = factor(measure, levels = c("mase", "msse", "crps"), labels = c("MASE", "MSSE", "CRPS")),
    series = factor(series, levels = c("Total", "Control areas", "Health boards")),
    model = factor(model, levels = c("Naïve", "ETS", "TSGLM", "GLM", "Ensemble")),
    week = factor(trunc((h - 1) / 7) + 1)
  ) |>
  group_by(week, model, measure, series) |>
  summarise(accuracy = mean(accuracy), .groups = "drop")


acc_summary |>
  ggplot(aes(x = week, y = accuracy, group = model, col = model)) +
  geom_line() +
  geom_point(size = .5) +
  facet_grid(measure ~ series, scales = "free_y") +
  labs(y = "Average accuracy", x = "Week ahead")
```

## Forecast accuracy: 43--84 days ahead
\fontsize{10}{10}\sf

```{r}
#| label: results_table_msse
# Set minimum in column to bold
set_to_bold <- function(table) {
  for (i in 3:6) {
    best <- which.min(table[[i]])
    table[[i]] <- sprintf(table[[i]], fmt = "%1.3f")
    table[[i]][best] <- cell_spec(table[[i]][best], bold = TRUE)
  }
  return(table)
}

msse1 <- rmsse |>
  filter(
    method %in% c("mint", "base"),
    model != "qcomb",
    h > 42
  ) |>
  mutate(
    model = factor(model,
      levels = c("naiveecdf", "ets", "iglm", "tscount", "ensemble"),
      labels = c("Naïve", "ETS", "GLM", "TSGLM", "Ensemble")
    ),
    method = factor(method, levels = c("base", "mint"), labels = c("Base", "MinT")),
  ) |>
  group_by(method, model, series) |>
  summarise(msse = mean(msse), .groups = "drop") |>
  pivot_wider(names_from = series, values_from = msse) |>
  select(Method = method, Model = model, Total, `Control areas`, `Health boards`, Bottom) |>
  set_to_bold()
kbl(msse1, booktabs = TRUE, escape = FALSE, align = "llrrrr") |>
  add_header_above(c(" " = 2, "MSSE" = 4)) |>
  kable_styling(latex_options = c("hold_position"))
```

## Forecast accuracy: 43--84 days ahead
\fontsize{10}{10}\sf

```{r}
#| label: results_table_mase
mase1 <- mase |>
  filter(
    method %in% c("mint", "base"),
    model != "qcomb",
    h > 42
  ) |>
  mutate(
    model = factor(model,
      levels = c("naiveecdf", "ets", "iglm", "tscount", "ensemble"),
      labels = c("Naïve", "ETS", "GLM", "TSGLM", "Ensemble")
    ),
    method = factor(method, levels = c("base", "mint"), labels = c("Base", "MinT"))
  ) |>
  group_by(method, model, series) |>
  summarise(mase = mean(mase), .groups = "drop") |>
  pivot_wider(names_from = series, values_from = mase) |>
  select(Method = method, Model = model, Total, `Control areas`, `Health boards`, Bottom) |>
  set_to_bold()
kbl(mase1, booktabs = TRUE, escape = FALSE, align = "llrrrr") |>
  add_header_above(c(" " = 2, "MASE" = 4)) |>
  kable_styling(latex_options = c("hold_position"))
```

## Forecast accuracy: 43--84 days ahead
\fontsize{10}{10}\sf

```{r}

crps1 <- readr::read_rds(here::here("results/crps.rds")) |>
  filter(
    method %in% c("mint", "base"),
    model != "qcomb",
    h > 42
  ) |>
  mutate(
    model = factor(model,
      levels = c("naiveecdf", "ets", "iglm", "tscount", "ensemble"),
      labels = c("Naïve", "ETS", "GLM", "TSGLM", "Ensemble")
    ),
    method = factor(method, levels = c("base", "mint"), labels = c("Base", "MinT"))
  ) |>
  group_by(method, model, series) |>
  summarise(crps = mean(crps), .groups = "drop") |>
  pivot_wider(names_from = series, values_from = crps) |>
  select(Method = method, Model = model, Total, `Control areas`, `Health boards`, Bottom) |>
  set_to_bold()
kbl(crps1, booktabs = TRUE, escape = FALSE, align = "llrrrr") |>
  add_header_above(c(" " = 2, "CRPS" = 4)) |>
  kable_styling(latex_options = c("hold_position"))
```

## Conclusions

* Ensemble mixture distributions give better forecasts than any component methods.
* Forecast reconciliation improves forecast accuracy, even when some component methods are quite poor.
* Forecast reconciliation allows coordinated planning and resource allocation.
